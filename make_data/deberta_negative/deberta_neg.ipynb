{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mon Aug 21 17:33:23 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.86.10              Driver Version: 535.86.10    CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 4090        On  | 00000000:01:00.0  On |                  Off |\n",
      "| 45%   50C    P8              25W / 450W |      3MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA GeForce RTX 4090        On  | 00000000:04:00.0 Off |                  Off |\n",
      "|  0%   48C    P8              25W / 450W |      3MiB / 24564MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 測試 deberta 的填空能力"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Paris\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.54"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DebertaForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"lsanochkin/deberta-large-feedback\")\n",
    "model = DebertaForMaskedLM.from_pretrained(\"lsanochkin/deberta-large-feedback\")\n",
    "\n",
    "inputs = tokenizer(\"The capital of France is [MASK].\", return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits\n",
    "\n",
    "# retrieve index of [MASK]\n",
    "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "print(tokenizer.decode(predicted_token_id))\n",
    "\n",
    "labels = tokenizer(\"The capital of France is Paris.\", return_tensors=\"pt\")[\"input_ids\"]\n",
    "# mask labels of non-[MASK] tokens\n",
    "labels = torch.where(inputs.input_ids == tokenizer.mask_token_id, labels, -100)\n",
    "\n",
    "outputs = model(**inputs, labels=labels)\n",
    "round(outputs.loss.item(), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def read_clothf_data():\n",
    "    path = '../../data/CLOTH-F/clean_cloth-f_dataset.json'\n",
    "    with open(path) as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "clothf_data = read_clothf_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'eating',\n",
      " 'distractors': ['working', 'preparing', 'thinking'],\n",
      " 'index': 0,\n",
      " 'sentence': 'I met Kurt Kampmeir of Success Motivation Incorporation for '\n",
      "             'breakfast. While we were _ ,Kurt askedme, \" John, what is your '\n",
      "             'plan for personal growth? Never at a loss for words, I tried to '\n",
      "             'find things in my life that might qualify for growth.I toldhim '\n",
      "             'about the many activities in which I was involved . '}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(clothf_data[\"test\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cate in clothf_data:\n",
    "    # print(i)\n",
    "    for s in clothf_data[cate]:\n",
    "        sent = s[\"sentence\"]\n",
    "        resent = sent.replace('_','[MASK]')\n",
    "        \n",
    "        inputs = tokenizer(resent, return_tensors=\"pt\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "        # retrieve index of [MASK]\n",
    "        mask_token_index = (inputs.input_ids == tokenizer.mask_token_id)[0].nonzero(as_tuple=True)[0]\n",
    "        k = 15\n",
    "\n",
    "        probs, indices = torch.topk(torch.softmax(logits[0, mask_token_index], -1), k)\n",
    "        s_probs = probs.squeeze(0)\n",
    "        s_indices = indices.squeeze(0)\n",
    "        \n",
    "        # 查看生成的單字\n",
    "        # for i,(p,t) in enumerate(zip(s_probs, s_indices)):\n",
    "        #     print(f\"{i} {p} {tokenizer.decode(t)}\")\n",
    "        \n",
    "        word_list = tokenizer.batch_decode(s_indices)\n",
    "        # print(word_list)\n",
    "        options = []\n",
    "        for j in word_list:\n",
    "            js = j.strip()\n",
    "            # print(js)\n",
    "            if js in [s[\"answer\"]]:\n",
    "                # print(js)\n",
    "                continue\n",
    "            elif js in s[\"distractors\"]:\n",
    "                continue\n",
    "            else:\n",
    "                options.append(js)\n",
    "        connect_option = s[\"distractors\"].copy()\n",
    "        connect_option.extend(options)\n",
    "        s[\"ranked_distractors\"] = connect_option[:6]\n",
    "        \n",
    "        # print(s[\"answer\"] ,s[\"distractors\"])\n",
    "        # print(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'eating',\n",
      " 'distractors': ['working', 'preparing', 'thinking'],\n",
      " 'index': 0,\n",
      " 'ranked_distractors': ['working',\n",
      "                        'preparing',\n",
      "                        'thinking',\n",
      "                        'chatting',\n",
      "                        'talking',\n",
      "                        'dining'],\n",
      " 'sentence': 'I met Kurt Kampmeir of Success Motivation Incorporation for '\n",
      "             'breakfast. While we were _ ,Kurt askedme, \" John, what is your '\n",
      "             'plan for personal growth? Never at a loss for words, I tried to '\n",
      "             'find things in my life that might qualify for growth.I toldhim '\n",
      "             'about the many activities in which I was involved . '}\n"
     ]
    }
   ],
   "source": [
    "pprint(clothf_data[\"test\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': 'plan',\n",
      " 'distractors': ['suggestion', 'demand', 'request'],\n",
      " 'index': 1,\n",
      " 'ranked_distractors': ['suggestion',\n",
      "                        'demand',\n",
      "                        'request',\n",
      "                        'idea',\n",
      "                        'ideal',\n",
      "                        'vision'],\n",
      " 'sentence': 'I met Kurt Kampmeir of Success Motivation Incorporation for '\n",
      "             'breakfast. While we were eating ,Kurt askedme, \" John, what is '\n",
      "             'your _ for personal growth? Never at a loss for words, I tried '\n",
      "             'to find things in my life that might qualify for growth.I '\n",
      "             'toldhim about the many activities in which I was involved . '}\n"
     ]
    }
   ],
   "source": [
    "pprint(clothf_data[\"test\"][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_json(test_sample):\n",
    "    with open(\"../../data/deberta_negative/clothf_negative.json\",\"w\",encoding=\"UTF-8\") as afile:\n",
    "        json.dump(test_sample,afile,ensure_ascii=False,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "quark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
